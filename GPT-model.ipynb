{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder (GPT) LLM From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary modules and setting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import fmmap as mmap\n",
    "import random\n",
    "import pickle\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Setting the GPU (mps on current MacOS) as the device, \n",
    "device =  'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "print(device) # Checking that mps is the device used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising tokenizer and setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT uncased tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 32\n",
    "batch_size = 16\n",
    "learning_rate= 3e-3\n",
    "num_epochs = 500\n",
    "eval_iters = 100\n",
    "n_embd = 32\n",
    "n_head = 1\n",
    "dropout = 0.2\n",
    "n_layer= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing large txt file in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChunkedTextIterableDataset(IterableDataset):\n",
    "    def __init__(self, filename, tokenizer, max_length=512):\n",
    "        self.filename = filename\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "            chunk = []\n",
    "            for line in f:\n",
    "                encoded_line = self.tokenizer.encode(line.strip(), add_special_tokens=True)\n",
    "                chunk.extend(encoded_line)\n",
    "\n",
    "                if len(chunk) >= self.max_length:\n",
    "                    yield chunk[:self.max_length]\n",
    "                    chunk = chunk[self.max_length:]\n",
    "\n",
    "            if chunk:  # Yielding final chunk\n",
    "                yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    # Pad all sequences to have the same length\n",
    "    padded_batch = pad_sequence([torch.tensor(seq) for seq in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    return padded_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(ChunkedTextIterableDataset('output_train.txt', tokenizer=tokenizer), \n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "val_dataset = DataLoader(ChunkedTextIterableDataset('output_val.txt', tokenizer=tokenizer),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        shuffle=False,\n",
    "                                                        collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "for i, batch in enumerate(train_dataset):\n",
    "    print(\"i:\", i, \"\\n\")\n",
    "    print(\"batch:\", batch, \"\\n\")\n",
    "    print(\"batch shape:\", batch.shape, \"\\n\")\n",
    "\n",
    "    a += 1\n",
    "    if a == 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward Layer (FNN) for non-linearity and regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\" simple linear layer followed by activation \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout) # Reduce overfit\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Head Class for input initialization into Key, Query and Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Mask\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = torch.matmul(q, k.transpose(-2, -1)) / (self.head_size ** 0.5)  # (B, T, T)\n",
    "\n",
    "        # Create a lower-triangular matrix of size (T, T) as mask\n",
    "        mask = torch.tril(torch.ones((T, T), device=x.device)).view(1, T, T)\n",
    "\n",
    "        # Apply the mask to the attention scores\n",
    "        wei = wei.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to get the attention probabilities\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        \n",
    "        # Apply dropout to attention weights\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        out = torch.matmul(wei, v)  # (B, T, head_size)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention Layer for capturing dependencies in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parrallel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = head_size\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range (num_heads)])\n",
    "        self.proj = nn.Linear(head_size*num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim = -1) # (B, T, F) --> (B, T, [h1,h1,h1,h1, ..., hn,hn,hn,hn])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block layer to sequence self-attention and FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd //n_head \n",
    "        self.sa = MultiHeadAttention(n_head,head_size) # Self-attention\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x+y) # Wrap around\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x+y) # Wrap around again\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT Language LLM Model Architecture, combining all above classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "     def __init__ (self, vocab_size):\n",
    "          super().__init__()\n",
    "          self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # Token embedding\n",
    "          self.positional_embedding = nn.Embedding(block_size, n_embd) # Positional Embedding\n",
    "          self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Creating our decoder (Block) layers\n",
    "          self.ln_f = nn.LayerNorm(n_embd) # Final layer norm\n",
    "          self.lm_head = nn.Linear(n_embd, vocab_size) # Final Linear Layer\n",
    "\n",
    "          self.apply(self._init_weights)\n",
    "\n",
    "     def _init_weights(self, module):\n",
    "          if isinstance(module, nn.Linear):\n",
    "               torch.nn.init.normal_(module.weight, mean = 0.0, std=0.02)\n",
    "               if module.bias is not None:\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "          elif isinstance(module, nn.Embedding):\n",
    "               torch.nn.init.normal_(module.weight, mean =0.0, std=0.02)\n",
    "\n",
    "     def forward(self, input_ids, labels = None):\n",
    "         # Getting the batch size (B) and seq_length (T) from the input_ids\n",
    "          B, T = input_ids.shape\n",
    "\n",
    "          #Token embeddings\n",
    "          token_embeddings = self.token_embedding_table(input_ids)              # (B, T, C)\n",
    "\n",
    "          # Positional Embedding\n",
    "          position_ids = torch.arange(0,T, dtype=torch.long, device = device)\n",
    "          positional_embeddings = self.positional_embedding(position_ids)       # (T, C)\n",
    "\n",
    "          # Combine token and position embeddings\n",
    "          hidden_states = token_embeddings + positional_embeddings.unsqueeze(0) # (B, T, C)\n",
    "\n",
    "          # Pass through transformer blocks\n",
    "          for block in self.blocks:\n",
    "               hidden_states = block(hidden_states)                             # (B, T, C)\n",
    "\n",
    "          #Final layer normalization\n",
    "          hidden_states= self.ln_f(hidden_states)                               #(B, T, C)\n",
    "\n",
    "          # Obtaining the logits (predictions for each token in the vocabulary)\n",
    "          logits = self.lm_head(hidden_states)                                  #(B, T, vocab_size)\n",
    "\n",
    "          # Obtaining Loss\n",
    "          loss_fn = nn.CrossEntropyLoss()\n",
    "          loss = loss_fn(logits.view(-1, self.lm_head.out_features), labels.view(-1))\n",
    "\n",
    "          return logits, loss\n",
    "\n",
    "     \n",
    "     def generate(self, index, max_new_tokens):\n",
    "          # using index of (B,T) array in current context\n",
    "          for _ in range(max_new_tokens): # Using _ as a placeholder to loop through max new tokens\n",
    "               #index_cond = index[:,-block_size]\n",
    "               logits, loss = self.forward(index)\n",
    "               logits = logits[:,-1,:]\n",
    "               probs = F.softmax(logits, dim= -1)\n",
    "               index_next = torch.multinomial(probs, num_samples=1)\n",
    "               index = torch.cat((index, index_next), dim =1)\n",
    "          return index\n",
    "\n",
    "model = GPTLanguageModel(vocab_size)\n",
    "# with open('model-01.pkl','rb') as f:\n",
    "#      model = pickle.load(f)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.AdamW(params = model.parameters(), lr = learning_rate) # Utilising Adam's AdamW variant to obtain weight decay/grad independence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating function to estimate loss during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train','test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X,Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(train_dataset):\n",
    "        input_ids = batch[:, :-1].to(device)\n",
    "        labels = batch[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits,loss = model(input_ids, labels = labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if epoch % eval_iters == 0 and batch_idx == 0:\n",
    "            print(f\"Loss at epoch {epoch}: {loss.item()}\")\n",
    "\n",
    "\n",
    "with open('model-01.pkl','wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(train_dataset):\n",
    "        input_ids = batch[:, :-1].to(device)\n",
    "        labels = batch[:, 1:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Running the model forward pass in mixed precision\n",
    "        with autocast():\n",
    "            logits, loss = model(input_ids, labels=labels)\n",
    "            loss = loss / accumulation_steps  # Normalize loss for gradient accumulation\n",
    "        \n",
    "        # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(train_dataset):\n",
    "            # Unscales gradients and calls or skips optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()  # Updates the scale for next iteration\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if epoch % eval_interval == 0 and batch_idx == 0:\n",
    "            print(f\"Loss at epoch {epoch}: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1,1), dtype = torch.long, device = device)\n",
    "generated_chars = tokenizer.decode(model.generate(context, max_new_tokens=150)[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
